{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balajikulkarni/The-one-with-Deep-Learning/blob/master/TransferLearning/TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykg64-UmWb4q",
        "colab_type": "text"
      },
      "source": [
        "## Transfer-Learning \n",
        "\n",
        "\n",
        "*   Load a pretrained ResNet50 model without last FC layer (include_top=False) for Imagenet.\n",
        "*   Fine-tune last few layers by adding GAP and SoftMax.\n",
        "\n",
        "*   Resize input images for CIFAR100 to larger ones as ResNet50 is trained with 224x224. \n",
        "*   Make BatchNormalization layers as trainable, this is needed because when using Frozen model,if the batch statistics (mean/variance) of frozen layers are used and if the target dataset is different from one which was used for training, this will result in degrading of accuracy (https://github.com/keras-team/keras/pull/9965)\n",
        "\n",
        "\n",
        "*   Add Cut-Out as an augmentation strategy.\n",
        "*   Trained for 15 epochs with BatchSize of 64 and reached 80.16% val-accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km_G0JdQyuDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4EMAANwO9E-",
        "colab_type": "code",
        "outputId": "a9d1a430-58a8-43de-d71f-434552b9ea89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_9TU3oKRvHw",
        "colab_type": "code",
        "outputId": "9e07e614-5183-4cf5-d04d-ba86582607a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "num_classes = 100\n",
        "nb_epochs = 10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "#Pre-process the data\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMsXhd1IW0nM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYelN4gNTPAY",
        "colab_type": "code",
        "outputId": "70dfe479-f6cb-494c-9d86-63a7b6eb75cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in resnet_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D())\n",
        "model.add(UpSampling2D())\n",
        "model.add(UpSampling2D())\n",
        "model.add(resnet_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(.25))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0HFOoen4R6",
        "colab_type": "code",
        "outputId": "645d7518-58ef-46a0-b75b-3197cd3f2ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "t=time.time()\n",
        "historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=64),\n",
        "                                  steps_per_epoch=x_train.shape[0] // 64,\n",
        "                                  epochs=15,\n",
        "                                  validation_data=(x_test, y_test))\n",
        "print('Training time: %s' % (t - time.time()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 2.0452 - acc: 0.3368Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 84s 8ms/sample - loss: 1.2927 - acc: 0.6762\n",
            "781/781 [==============================] - 1150s 1s/step - loss: 2.0442 - acc: 0.3371 - val_loss: 1.0963 - val_acc: 0.6762\n",
            "Epoch 2/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 1.1737 - acc: 0.6529Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.9556 - acc: 0.7398\n",
            "781/781 [==============================] - 1130s 1s/step - loss: 1.1738 - acc: 0.6530 - val_loss: 0.8594 - val_acc: 0.7398\n",
            "Epoch 3/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.9942 - acc: 0.7071Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.7662 - acc: 0.7647\n",
            "781/781 [==============================] - 1129s 1s/step - loss: 0.9944 - acc: 0.7071 - val_loss: 0.7813 - val_acc: 0.7647\n",
            "Epoch 4/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.8907 - acc: 0.7375Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.8552 - acc: 0.7789\n",
            "781/781 [==============================] - 1130s 1s/step - loss: 0.8908 - acc: 0.7375 - val_loss: 0.7480 - val_acc: 0.7789\n",
            "Epoch 5/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.8031 - acc: 0.7608Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.7097 - acc: 0.7820\n",
            "781/781 [==============================] - 1128s 1s/step - loss: 0.8033 - acc: 0.7608 - val_loss: 0.7290 - val_acc: 0.7820\n",
            "Epoch 6/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.7468 - acc: 0.7772Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.6906 - acc: 0.7899\n",
            "781/781 [==============================] - 1130s 1s/step - loss: 0.7468 - acc: 0.7772 - val_loss: 0.7078 - val_acc: 0.7899\n",
            "Epoch 7/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.7051 - acc: 0.7964Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.6924 - acc: 0.7851\n",
            "781/781 [==============================] - 1130s 1s/step - loss: 0.7051 - acc: 0.7963 - val_loss: 0.7299 - val_acc: 0.7851\n",
            "Epoch 8/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.6624 - acc: 0.8018Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.7569 - acc: 0.7669\n",
            "781/781 [==============================] - 1129s 1s/step - loss: 0.6629 - acc: 0.8018 - val_loss: 0.8309 - val_acc: 0.7669\n",
            "Epoch 9/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.6206 - acc: 0.8097Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.7751 - acc: 0.7945\n",
            "781/781 [==============================] - 1129s 1s/step - loss: 0.6210 - acc: 0.8097 - val_loss: 0.7240 - val_acc: 0.7945\n",
            "Epoch 10/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.5974 - acc: 0.8174Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.7272 - acc: 0.7948\n",
            "781/781 [==============================] - 1129s 1s/step - loss: 0.5971 - acc: 0.8174 - val_loss: 0.7143 - val_acc: 0.7948\n",
            "Epoch 11/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.5627 - acc: 0.8266Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 83s 8ms/sample - loss: 0.7197 - acc: 0.7974\n",
            "781/781 [==============================] - 1132s 1s/step - loss: 0.5627 - acc: 0.8266 - val_loss: 0.7174 - val_acc: 0.7974\n",
            "Epoch 12/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.5340 - acc: 0.8367Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.9034 - acc: 0.7925\n",
            "781/781 [==============================] - 1129s 1s/step - loss: 0.5340 - acc: 0.8367 - val_loss: 0.7339 - val_acc: 0.7925\n",
            "Epoch 13/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.5214 - acc: 0.8380Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.8135 - acc: 0.7970\n",
            "781/781 [==============================] - 1131s 1s/step - loss: 0.5214 - acc: 0.8380 - val_loss: 0.7300 - val_acc: 0.7970\n",
            "Epoch 14/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.4838 - acc: 0.8520Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.9364 - acc: 0.7970\n",
            "781/781 [==============================] - 1130s 1s/step - loss: 0.4838 - acc: 0.8520 - val_loss: 0.7458 - val_acc: 0.7970\n",
            "Epoch 15/15\n",
            "780/781 [============================>.] - ETA: 1s - loss: 0.4779 - acc: 0.8554Epoch 1/15\n",
            "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 82s 8ms/sample - loss: 0.7969 - acc: 0.8016\n",
            "781/781 [==============================] - 1128s 1s/step - loss: 0.4779 - acc: 0.8554 - val_loss: 0.7358 - val_acc: 0.8016\n",
            "Training time: -16964.41730928421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WsUOYLgqVBw",
        "colab_type": "code",
        "outputId": "69da12cf-2ac1-4535-bf73-bd810444aadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d (UpSampling2D) multiple                  0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 multiple                  0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 multiple                  0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  524544    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo multiple                  1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  25700     \n",
            "=================================================================\n",
            "Total params: 24,138,980\n",
            "Trainable params: 603,876\n",
            "Non-trainable params: 23,535,104\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
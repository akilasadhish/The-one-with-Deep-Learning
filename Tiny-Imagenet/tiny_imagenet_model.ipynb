{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_NJMNp4F0kn",
        "colab_type": "code",
        "outputId": "cb7bad9c-1228-49a3-9c27-1639c8232288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive,files\n",
        "drive.mount('/content/drive')   \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZNWOF6r3qA8",
        "colab_type": "code",
        "outputId": "332b7384-3323-4c8a-a059-6b38ade10587",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-94dec2f6-5244-4ab4-a24f-a07942c97818\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-94dec2f6-5244-4ab4-a24f-a07942c97818\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving resnet.py to resnet.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'resnet.py': b'from __future__ import division\\r\\n\\r\\nimport six\\r\\nfrom keras.models import Model\\r\\nfrom keras.layers import (\\r\\n    Input,\\r\\n    Activation,\\r\\n    Dense,\\r\\n    Flatten,\\r\\n    SeparableConv2D\\r\\n)\\r\\nfrom keras.layers.convolutional import (\\r\\n    Conv2D,\\r\\n    MaxPooling2D,\\r\\n    AveragePooling2D,\\r\\n    ZeroPadding2D\\r\\n)\\r\\nfrom keras.layers.pooling import GlobalAveragePooling2D, GlobalMaxPooling2D\\r\\nfrom keras.layers.merge import add\\r\\nfrom keras.layers.normalization import BatchNormalization\\r\\nfrom keras.regularizers import l2\\r\\nfrom keras import backend as K\\r\\nfrom keras_applications.imagenet_utils import _obtain_input_shape\\r\\n\\r\\n\\r\\ndef _handle_dim_ordering():\\r\\n    global ROW_AXIS\\r\\n    global COL_AXIS\\r\\n    global CHANNEL_AXIS\\r\\n    if K.image_dim_ordering() == \\'tf\\':\\r\\n        ROW_AXIS = 1\\r\\n        COL_AXIS = 2\\r\\n        CHANNEL_AXIS = 3\\r\\n    else:\\r\\n        CHANNEL_AXIS = 1\\r\\n        ROW_AXIS = 2\\r\\n        COL_AXIS = 3\\r\\n\\r\\nclass ResnetBuilder(object):\\r\\n    @staticmethod\\r\\n    def identity_conv_block(input,filters,stride=(1,1)):\\r\\n        identity = input\\r\\n        inp = Conv2D(filters=filters, kernel_size= (1,1), strides=stride,padding=\\'same\\')(input)\\r\\n        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\\r\\n        inp = Activation(\"relu\")(inp)\\r\\n\\r\\n        inp = Conv2D(filters=filters, kernel_size = (3,3), strides=stride, padding=\\'same\\')(inp)\\r\\n        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\\r\\n        inp = Activation(\\'relu\\')(inp)\\r\\n\\r\\n        inp = Conv2D(filters=filters, kernel_size = (1,1), strides=stride,padding=\\'same\\')(inp)\\r\\n        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\\r\\n\\r\\n        out = add([inp, identity])\\r\\n        out = Activation(\\'relu\\')(out)\\r\\n\\r\\n        return out\\r\\n\\r\\n    @staticmethod\\r\\n    def conv_block(input,filters,stride=(1,1)):\\r\\n        shortcut = input\\r\\n        inp = Conv2D(filters=filters, kernel_size =(1,1), strides=stride,padding=\\'same\\')(input)\\r\\n        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\\r\\n        inp = Activation(\\'relu\\')(inp)\\r\\n\\r\\n        inp = Conv2D(filters=filters, kernel_size =(3,3), strides=stride, padding=\\'same\\')(inp)\\r\\n        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\\r\\n        inp = Activation(\\'relu\\')(inp)\\r\\n\\r\\n        inp = Conv2D(filters=filters, kernel_size =(1,1), strides=stride)(inp)\\r\\n        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\\r\\n\\r\\n        #To match the residual with input dimensions\\r\\n        shortcut = Conv2D(filters=filters, kernel_size =(1,1), strides=stride,padding=\\'same\\')(shortcut)\\r\\n        shortcut = BatchNormalization(axis=CHANNEL_AXIS)(shortcut)\\r\\n\\r\\n        out = add([inp, shortcut])\\r\\n        out = Activation(\\'relu\\')(out)\\r\\n\\r\\n        return out\\r\\n\\r\\n    @staticmethod\\r\\n    def build(input_shape, num_outputs, repetitions):\\r\\n        \"\"\"Builds a custom ResNet like architecture.\\r\\n        Args:\\r\\n            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\\r\\n            num_outputs: The number of outputs at final softmax layer\\r\\n            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\\r\\n                The original paper used basic_block for layers < 50\\r\\n            repetitions: Number of repetitions of various block units.\\r\\n                At each block unit, the number of filters are doubled and the input size is halved\\r\\n        Returns:\\r\\n            The keras `Model`.\\r\\n        \"\"\"\\r\\n        _handle_dim_ordering()\\r\\n        if len(input_shape) != 3:\\r\\n            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\\r\\n\\r\\n        # Permute dimension order if necessary\\r\\n        if K.image_dim_ordering() == \\'tf\\':\\r\\n            input_shape = (input_shape[1], input_shape[2], input_shape[0])\\r\\n\\r\\n        inp = Input(shape=input_shape)\\r\\n        conv1 = Conv2D(filters=64, kernel_size=(5,5), strides=(1, 1),padding=\\'same\\')(inp)\\r\\n        bn1 = BatchNormalization(axis=CHANNEL_AXIS)(conv1)\\r\\n        act1 = Activation(\\'relu\\')(bn1)\\r\\n        pool_op = MaxPooling2D(pool_size=(3,3), strides=(2, 2), padding=\\'same\\')(act1)\\r\\n\\r\\n        block = pool_op\\r\\n\\r\\n        # [[1x1,64],[3x3,64],[1x1,64]]x3\\r\\n        # [[1x1,128],[3x3,128],[1x1,128]]x4\\r\\n        # [[1x1,256],[3x3,256],[1x1,256]]x6\\r\\n        # Opt:[[1x1,512],[3x3,512],[1x1,512]]x3\\r\\n        filters = 64\\r\\n        init_strides = (1,1)\\r\\n        for i in range(0, len(repetitions)):\\r\\n            block = ResnetBuilder.conv_block(block, filters, init_strides)\\r\\n\\r\\n            identity_block_length = repetitions[i]\\r\\n            for j in range(0, identity_block_length-1):\\r\\n               block = ResnetBuilder.identity_conv_block(block, filters, (1,1))\\r\\n\\r\\n            filters = filters * 2;\\r\\n\\r\\n        # Classifier block\\r\\n        block = BatchNormalization(axis=CHANNEL_AXIS)(block)\\r\\n        block = Activation(\"relu\")(block)\\r\\n\\r\\n        block_shape = K.int_shape(block)\\r\\n\\r\\n        sep2D_op = SeparableConv2D(block_shape[CHANNEL_AXIS] // 2, (3, 3))(block)\\r\\n        sep2D_op = SeparableConv2D(num_outputs, (3,3))(sep2D_op)\\r\\n\\r\\n        # Try with GAP first, if accuracy not reaching target, try with GMP?\\r\\n        pool_gap =  GlobalAveragePooling2D()(sep2D_op)\\r\\n        output = Activation(\\'softmax\\')(pool_gap)\\r\\n\\r\\n        model = Model(inputs=inp, outputs=output)\\r\\n        return model\\r\\n\\r\\n    #Experiment with custom number of repetitions to avoid OOM\\r\\n    @staticmethod\\r\\n    def build_resnet_modified(input_shape, num_outputs):\\r\\n        return ResnetBuilder.build(input_shape, num_outputs, [3,4,6])\\r\\n\\r\\n    @staticmethod\\r\\n    def build_resnet_modified_50(input_shape, num_outputs):\\r\\n        return ResnetBuilder.build(input_shape, num_outputs, [3, 4, 6, 3])\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvp4M1bB2vN3",
        "colab_type": "code",
        "outputId": "680a768b-8f39-4324-9d81-2bb509bf33f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2503
        }
      },
      "source": [
        "!cat resnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from __future__ import division\r\n",
            "\r\n",
            "import six\r\n",
            "from keras.models import Model\r\n",
            "from keras.layers import (\r\n",
            "    Input,\r\n",
            "    Activation,\r\n",
            "    Dense,\r\n",
            "    Flatten,\r\n",
            "    SeparableConv2D\r\n",
            ")\r\n",
            "from keras.layers.convolutional import (\r\n",
            "    Conv2D,\r\n",
            "    MaxPooling2D,\r\n",
            "    AveragePooling2D,\r\n",
            "    ZeroPadding2D\r\n",
            ")\r\n",
            "from keras.layers.pooling import GlobalAveragePooling2D, GlobalMaxPooling2D\r\n",
            "from keras.layers.merge import add\r\n",
            "from keras.layers.normalization import BatchNormalization\r\n",
            "from keras.regularizers import l2\r\n",
            "from keras import backend as K\r\n",
            "from keras_applications.imagenet_utils import _obtain_input_shape\r\n",
            "\r\n",
            "\r\n",
            "def _handle_dim_ordering():\r\n",
            "    global ROW_AXIS\r\n",
            "    global COL_AXIS\r\n",
            "    global CHANNEL_AXIS\r\n",
            "    if K.image_dim_ordering() == 'tf':\r\n",
            "        ROW_AXIS = 1\r\n",
            "        COL_AXIS = 2\r\n",
            "        CHANNEL_AXIS = 3\r\n",
            "    else:\r\n",
            "        CHANNEL_AXIS = 1\r\n",
            "        ROW_AXIS = 2\r\n",
            "        COL_AXIS = 3\r\n",
            "\r\n",
            "class ResnetBuilder(object):\r\n",
            "    @staticmethod\r\n",
            "    def identity_conv_block(input,filters,stride=(1,1)):\r\n",
            "        identity = input\r\n",
            "        inp = Conv2D(filters=filters, kernel_size= (1,1), strides=stride,padding='same')(input)\r\n",
            "        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\r\n",
            "        inp = Activation(\"relu\")(inp)\r\n",
            "\r\n",
            "        inp = Conv2D(filters=filters, kernel_size = (3,3), strides=stride, padding='same')(inp)\r\n",
            "        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\r\n",
            "        inp = Activation('relu')(inp)\r\n",
            "\r\n",
            "        inp = Conv2D(filters=filters, kernel_size = (1,1), strides=stride,padding='same')(inp)\r\n",
            "        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\r\n",
            "\r\n",
            "        out = add([inp, identity])\r\n",
            "        out = Activation('relu')(out)\r\n",
            "\r\n",
            "        return out\r\n",
            "\r\n",
            "    @staticmethod\r\n",
            "    def conv_block(input,filters,stride=(1,1)):\r\n",
            "        shortcut = input\r\n",
            "        inp = Conv2D(filters=filters, kernel_size =(1,1), strides=stride,padding='same')(input)\r\n",
            "        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\r\n",
            "        inp = Activation('relu')(inp)\r\n",
            "\r\n",
            "        inp = Conv2D(filters=filters, kernel_size =(3,3), strides=stride, padding='same')(inp)\r\n",
            "        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\r\n",
            "        inp = Activation('relu')(inp)\r\n",
            "\r\n",
            "        inp = Conv2D(filters=filters, kernel_size =(1,1), strides=stride)(inp)\r\n",
            "        inp = BatchNormalization(axis=CHANNEL_AXIS)(inp)\r\n",
            "\r\n",
            "        #To match the residual with input dimensions\r\n",
            "        shortcut = Conv2D(filters=filters, kernel_size =(1,1), strides=stride,padding='same')(shortcut)\r\n",
            "        shortcut = BatchNormalization(axis=CHANNEL_AXIS)(shortcut)\r\n",
            "\r\n",
            "        out = add([inp, shortcut])\r\n",
            "        out = Activation('relu')(out)\r\n",
            "\r\n",
            "        return out\r\n",
            "\r\n",
            "    @staticmethod\r\n",
            "    def build(input_shape, num_outputs, repetitions):\r\n",
            "        \"\"\"Builds a custom ResNet like architecture.\r\n",
            "        Args:\r\n",
            "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\r\n",
            "            num_outputs: The number of outputs at final softmax layer\r\n",
            "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\r\n",
            "                The original paper used basic_block for layers < 50\r\n",
            "            repetitions: Number of repetitions of various block units.\r\n",
            "                At each block unit, the number of filters are doubled and the input size is halved\r\n",
            "        Returns:\r\n",
            "            The keras `Model`.\r\n",
            "        \"\"\"\r\n",
            "        _handle_dim_ordering()\r\n",
            "        if len(input_shape) != 3:\r\n",
            "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\r\n",
            "\r\n",
            "        # Permute dimension order if necessary\r\n",
            "        if K.image_dim_ordering() == 'tf':\r\n",
            "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\r\n",
            "\r\n",
            "        inp = Input(shape=input_shape)\r\n",
            "        conv1 = Conv2D(filters=64, kernel_size=(5,5), strides=(1, 1),padding='same')(inp)\r\n",
            "        bn1 = BatchNormalization(axis=CHANNEL_AXIS)(conv1)\r\n",
            "        act1 = Activation('relu')(bn1)\r\n",
            "        pool_op = MaxPooling2D(pool_size=(3,3), strides=(2, 2), padding='same')(act1)\r\n",
            "\r\n",
            "        block = pool_op\r\n",
            "\r\n",
            "        # [[1x1,64],[3x3,64],[1x1,64]]x3\r\n",
            "        # [[1x1,128],[3x3,128],[1x1,128]]x4\r\n",
            "        # [[1x1,256],[3x3,256],[1x1,256]]x6\r\n",
            "        # Opt:[[1x1,512],[3x3,512],[1x1,512]]x3\r\n",
            "        filters = 64\r\n",
            "        init_strides = (1,1)\r\n",
            "        for i in range(0, len(repetitions)):\r\n",
            "            block = ResnetBuilder.conv_block(block, filters, init_strides)\r\n",
            "\r\n",
            "            identity_block_length = repetitions[i]\r\n",
            "            for j in range(0, identity_block_length-1):\r\n",
            "               block = ResnetBuilder.identity_conv_block(block, filters, (1,1))\r\n",
            "\r\n",
            "            filters = filters * 2;\r\n",
            "\r\n",
            "        # Classifier block\r\n",
            "        block = BatchNormalization(axis=CHANNEL_AXIS)(block)\r\n",
            "        block = Activation(\"relu\")(block)\r\n",
            "\r\n",
            "        block_shape = K.int_shape(block)\r\n",
            "\r\n",
            "        sep2D_op = SeparableConv2D(block_shape[CHANNEL_AXIS] // 2, (3, 3))(block)\r\n",
            "        sep2D_op = SeparableConv2D(num_outputs, (3,3))(sep2D_op)\r\n",
            "\r\n",
            "        # Try with GAP first, if accuracy not reaching target, try with GMP?\r\n",
            "        pool_gap =  GlobalAveragePooling2D()(sep2D_op)\r\n",
            "        output = Activation('softmax')(pool_gap)\r\n",
            "\r\n",
            "        model = Model(inputs=inp, outputs=output)\r\n",
            "        return model\r\n",
            "\r\n",
            "    @staticmethod\r\n",
            "    def build_resnet_modified(input_shape, num_outputs):\r\n",
            "        return ResnetBuilder.build(input_shape, num_outputs, [3,4,6])\r\n",
            "\r\n",
            "    @staticmethod\r\n",
            "    def build_resnet_modified_50(input_shape, num_outputs):\r\n",
            "        return ResnetBuilder.build(input_shape, num_outputs, [3, 4, 6, 3])\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3aNMmsFG_G8",
        "colab_type": "code",
        "outputId": "d569d612-5b50-4e79-b604-7c22f861eafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils,multi_gpu_model\n",
        "from keras.callbacks import *\n",
        "from keras.layers import  Flatten, GlobalAveragePooling2D\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import *\n",
        "from imgaug import augmenters as iaa\n",
        "from keras import backend as K\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import resnet\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "K.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUKnMO3KIFPn",
        "colab_type": "code",
        "outputId": "cb50e4f2-6679-431d-b29e-3e5d74cb784a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/dataset\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tiny-imagenet-200.zip\t\t       weights.epochs:008-val_acc:0.503.hdf5\n",
            "weights.epochs:002-val_acc:0.468.hdf5  weights.epochs:018-val_acc:0.472.hdf5\n",
            "weights.epochs:002-val_acc:0.484.hdf5  weights.epochs:020-val_acc:0.445.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdDzpFnSIzUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip \"/content/drive/My Drive/dataset/tiny-imagenet-200.zip\" -d \"/content/drive/My Drive/dataset\"\n",
        "!unzip -qq \"/content/drive/My Drive/dataset/tiny-imagenet-200.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXGZr0lfVNe4",
        "colab_type": "text"
      },
      "source": [
        "## Begin with 32x32 training images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Itg69vFzO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_DIR = 'tiny-imagenet-200/train'\n",
        "VALIDATION_DATA_DIR = 'tiny-imagenet-200/val'\n",
        "\n",
        "img_rows = 32\n",
        "img_cols = 32\n",
        "img_channels = 3\n",
        "\n",
        "nb_classes = 200\n",
        "\n",
        "filepath = \"/content/drive/My Drive/dataset/weights.epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "  \n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                            monitor='val_acc',\n",
        "                            verbose=1,\n",
        "                            save_best_only=True,\n",
        "                            mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor = np.sqrt(0.1),\n",
        "                              patience=5,\n",
        "                              min_lr=0.5e-6,\n",
        "                              verbose=1)\n",
        "\n",
        "callbacks_list = [checkpoint,reduce_lr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-v5kOwUDp4Z",
        "colab_type": "code",
        "outputId": "4e5c7e02-d7c0-4873-82df-a42e937cf33f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "'''\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=[0.5, 1.30],\n",
        "                                   brightness_range=[0.5, 1.5],\n",
        "                                   rotation_range=40)\n",
        "'''\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR, \n",
        "        target_size=(img_rows,img_cols),\n",
        "        color_mode = 'rgb',\n",
        "        batch_size=256,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed = 42)\n",
        "\n",
        "\n",
        "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "        val_data, \n",
        "        directory='./tiny-imagenet-200/val/images/', \n",
        "        x_col='File', y_col='Class', \n",
        "        target_size=(32, 32),\n",
        "        color_mode='rgb', \n",
        "        class_mode='categorical',\n",
        "        batch_size=256, \n",
        "        shuffle=True, \n",
        "        seed=42)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgwMc7naVswT",
        "colab_type": "text"
      },
      "source": [
        "## **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K81zCi8lBqx",
        "colab_type": "code",
        "outputId": "b9e80a34-f8e9-4ccb-df94-8ffcff288457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5362
        }
      },
      "source": [
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "model = resnet.ResnetBuilder.build_resnet_modified((img_channels,img_rows,img_cols), nb_classes)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   4864        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 64)   4160        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 64)   4160        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   4160        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 16, 64)   0           batch_normalization_4[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   4160        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   4160        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   4160        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   4160        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 64)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  8320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  16512       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           batch_normalization_14[0][0]     \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 128)  16512       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 128)  147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 128)  512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 128)  16512       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 128)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 128)  16512       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 128)  147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 128)  512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 128)  16512       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 128)  512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 128)  0           batch_normalization_21[0][0]     \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 128)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 128)  16512       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 128)  512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 128)  512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 128)  16512       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 128)  512         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 16, 16, 128)  0           batch_normalization_24[0][0]     \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 128)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 256)  33024       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 256)  1024        conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 256)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 256)  590080      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 256)  1024        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 256)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 256)  65792       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 256)  33024       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 256)  1024        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 256)  0           batch_normalization_27[0][0]     \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 256)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 256)  65792       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 256)  1024        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 256)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 256)  590080      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 256)  1024        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 256)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 256)  65792       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 256)  1024        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 256)  0           batch_normalization_31[0][0]     \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 256)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 16, 16, 256)  65792       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 16, 16, 256)  1024        conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 256)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 256)  590080      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 16, 16, 256)  1024        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 256)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 256)  65792       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 256)  1024        conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 256)  0           batch_normalization_34[0][0]     \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 256)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 256)  65792       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 256)  1024        conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 256)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 256)  590080      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 256)  1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 256)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 256)  65792       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 256)  1024        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 16, 256)  0           batch_normalization_37[0][0]     \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 256)  0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 256)  65792       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 256)  1024        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 256)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 256)  590080      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 256)  1024        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 256)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 256)  65792       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 256)  1024        conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 256)  0           batch_normalization_40[0][0]     \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 256)  0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 16, 16, 256)  65792       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 256)  1024        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 256)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 256)  590080      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 256)  1024        conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 256)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 256)  65792       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 256)  1024        conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 256)  0           batch_normalization_43[0][0]     \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 256)  0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 256)  1024        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 256)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 14, 14, 128)  35200       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 12, 12, 200)  26952       separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 200)          0           separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 200)          0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 5,289,672\n",
            "Trainable params: 5,274,696\n",
            "Non-trainable params: 14,976\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z46xf3ocERtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_TEST=validation_generator.n//validation_generator.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE0YpTOugx2c",
        "colab_type": "code",
        "outputId": "d8b3cc61-9e9c-41d7-9b63-482fd78a6968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "img_rows, img_cols = 32,32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "'''\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=[0.5, 1.30],\n",
        "                                   brightness_range=[0.5, 1.5],\n",
        "                                   rotation_range=35)\n",
        "'''\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR, \n",
        "        target_size=(img_rows,img_cols),\n",
        "        color_mode = 'rgb',\n",
        "        batch_size=256,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed = 42)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "        val_data, \n",
        "        directory='./tiny-imagenet-200/val/images/', \n",
        "        x_col='File', y_col='Class', \n",
        "        target_size=(img_rows, img_cols),\n",
        "        color_mode='rgb', \n",
        "        class_mode='categorical',\n",
        "        batch_size=256, \n",
        "        shuffle=True, \n",
        "        seed=42)\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_TEST=validation_generator.n//validation_generator.batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G7vH8eji6Ph",
        "colab_type": "text"
      },
      "source": [
        "### Train on 32x32 dataset  (one session training logs of 30 epochs got overwritten by mistake,as I re-ran the same cell after loading model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3nv5ZCHOp2N",
        "colab_type": "code",
        "outputId": "6bcc6f9a-d640-4cf8-ebec-b87d5a9e7de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1566
        }
      },
      "source": [
        "optimizer = Adam(lr= 0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor = 0.2,\n",
        "                              patience=5,\n",
        "                              min_lr=0.5e-6,\n",
        "                              verbose=1)\n",
        "\n",
        "model = resnet.ResnetBuilder.build_resnet_modified((img_channels,img_rows,img_cols), nb_classes)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('/content/drive/My Drive/dataset/weights.epochs:009-val_acc:0.414.hdf5')\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_TEST,\n",
        "                    epochs=20,\n",
        "                    callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "390/390 [==============================] - 493s 1s/step - loss: 3.3693 - acc: 0.2680 - val_loss: 2.9863 - val_acc: 0.3457\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.34565, saving model to /content/drive/My Drive/dataset/weights.epochs:001-val_acc:0.346.hdf5\n",
            "Epoch 2/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.9629 - acc: 0.3180 - val_loss: 3.1274 - val_acc: 0.3193\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.34565\n",
            "Epoch 3/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.8665 - acc: 0.3328 - val_loss: 3.0721 - val_acc: 0.3409\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.34565\n",
            "Epoch 4/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.8078 - acc: 0.3479 - val_loss: 3.0578 - val_acc: 0.3367\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.34565\n",
            "Epoch 5/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.7638 - acc: 0.3564 - val_loss: 2.9213 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.34565 to 0.35827, saving model to /content/drive/My Drive/dataset/weights.epochs:005-val_acc:0.358.hdf5\n",
            "Epoch 6/20\n",
            "390/390 [==============================] - 479s 1s/step - loss: 2.7210 - acc: 0.3643 - val_loss: 3.0178 - val_acc: 0.3641\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.35827 to 0.36412, saving model to /content/drive/My Drive/dataset/weights.epochs:006-val_acc:0.364.hdf5\n",
            "Epoch 7/20\n",
            "390/390 [==============================] - 479s 1s/step - loss: 2.6781 - acc: 0.3725 - val_loss: 3.1784 - val_acc: 0.3354\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.36412\n",
            "Epoch 8/20\n",
            "390/390 [==============================] - 479s 1s/step - loss: 2.6490 - acc: 0.3785 - val_loss: 3.1736 - val_acc: 0.3357\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.36412\n",
            "Epoch 9/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.6193 - acc: 0.3831 - val_loss: 3.1684 - val_acc: 0.3451\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.36412\n",
            "Epoch 10/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.5767 - acc: 0.3910 - val_loss: 3.1131 - val_acc: 0.3497\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.36412\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
            "Epoch 11/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.4185 - acc: 0.4210 - val_loss: 2.9492 - val_acc: 0.3841\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.36412 to 0.38413, saving model to /content/drive/My Drive/dataset/weights.epochs:011-val_acc:0.384.hdf5\n",
            "Epoch 12/20\n",
            "390/390 [==============================] - 482s 1s/step - loss: 2.3656 - acc: 0.4335 - val_loss: 2.8835 - val_acc: 0.3993\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.38413 to 0.39932, saving model to /content/drive/My Drive/dataset/weights.epochs:012-val_acc:0.399.hdf5\n",
            "Epoch 13/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.3440 - acc: 0.4374 - val_loss: 2.9799 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.39932\n",
            "Epoch 14/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.3242 - acc: 0.4421 - val_loss: 2.9942 - val_acc: 0.3922\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.39932\n",
            "Epoch 15/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.3017 - acc: 0.4471 - val_loss: 2.8767 - val_acc: 0.3984\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.39932\n",
            "Epoch 16/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.2852 - acc: 0.4484 - val_loss: 3.0113 - val_acc: 0.3896\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.39932\n",
            "Epoch 17/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.2675 - acc: 0.4539 - val_loss: 3.0501 - val_acc: 0.3866\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.39932\n",
            "Epoch 18/20\n",
            "390/390 [==============================] - 482s 1s/step - loss: 2.2627 - acc: 0.4534 - val_loss: 2.8827 - val_acc: 0.3979\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.39932\n",
            "Epoch 19/20\n",
            "390/390 [==============================] - 483s 1s/step - loss: 2.2440 - acc: 0.4567 - val_loss: 2.9120 - val_acc: 0.4060\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.39932 to 0.40599, saving model to /content/drive/My Drive/dataset/weights.epochs:019-val_acc:0.406.hdf5\n",
            "Epoch 20/20\n",
            "390/390 [==============================] - 482s 1s/step - loss: 2.2362 - acc: 0.4597 - val_loss: 3.1587 - val_acc: 0.3839\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.40599\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fe7e22a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWypnne1iGHB",
        "colab_type": "code",
        "outputId": "b4da1ade-96a0-4727-e0ba-dcdaf0a07e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1478
        }
      },
      "source": [
        "\n",
        "img_rows, img_cols = 32,32\n",
        "\n",
        "#train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR, \n",
        "        target_size=(img_rows,img_cols),\n",
        "        color_mode = 'rgb',\n",
        "        batch_size=256,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed = 42)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "        val_data, \n",
        "        directory='./tiny-imagenet-200/val/images/', \n",
        "        x_col='File', y_col='Class', \n",
        "        target_size=(img_rows, img_cols),\n",
        "        color_mode='rgb', \n",
        "        class_mode='categorical',\n",
        "        batch_size=256, \n",
        "        shuffle=True, \n",
        "        seed=42)\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_TEST=validation_generator.n//validation_generator.batch_size\n",
        "model.load_weights('/content/drive/My Drive/dataset/weights.epochs:009-val_acc:0.419.hdf5\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_TEST,\n",
        "                    epochs=20,\n",
        "                    callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "390/390 [==============================] - 482s 1s/step - loss: 2.1617 - acc: 0.4740 - val_loss: 3.0405 - val_acc: 0.3980\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.40599\n",
            "Epoch 2/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.1467 - acc: 0.4777 - val_loss: 2.9733 - val_acc: 0.4065\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.40599 to 0.40651, saving model to /content/drive/My Drive/dataset/weights.epochs:002-val_acc:0.407.hdf5\n",
            "Epoch 3/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.1386 - acc: 0.4767 - val_loss: 2.9545 - val_acc: 0.4058\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.40651\n",
            "Epoch 4/20\n",
            "390/390 [==============================] - 478s 1s/step - loss: 2.1272 - acc: 0.4800 - val_loss: 3.0324 - val_acc: 0.3982\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.40651\n",
            "Epoch 5/20\n",
            "390/390 [==============================] - 478s 1s/step - loss: 2.1221 - acc: 0.4818 - val_loss: 2.9121 - val_acc: 0.4117\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.40651 to 0.41174, saving model to /content/drive/My Drive/dataset/weights.epochs:005-val_acc:0.412.hdf5\n",
            "Epoch 6/20\n",
            "390/390 [==============================] - 479s 1s/step - loss: 2.1156 - acc: 0.4830 - val_loss: 3.0047 - val_acc: 0.4006\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.41174\n",
            "Epoch 7/20\n",
            "390/390 [==============================] - 479s 1s/step - loss: 2.1172 - acc: 0.4819 - val_loss: 3.0261 - val_acc: 0.3970\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.41174\n",
            "Epoch 8/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.1024 - acc: 0.4852 - val_loss: 2.9619 - val_acc: 0.4113\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.41174\n",
            "Epoch 9/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.1002 - acc: 0.4871 - val_loss: 2.9751 - val_acc: 0.4080\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.41174\n",
            "Epoch 10/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.0986 - acc: 0.4875 - val_loss: 3.0277 - val_acc: 0.4018\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.41174\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.\n",
            "Epoch 11/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0810 - acc: 0.4892 - val_loss: 2.9876 - val_acc: 0.4093\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.41174\n",
            "Epoch 12/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0665 - acc: 0.4937 - val_loss: 3.0506 - val_acc: 0.4017\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.41174\n",
            "Epoch 13/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0627 - acc: 0.4943 - val_loss: 2.9417 - val_acc: 0.4137\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.41174 to 0.41369, saving model to /content/drive/My Drive/dataset/weights.epochs:013-val_acc:0.414.hdf5\n",
            "Epoch 14/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.0690 - acc: 0.4925 - val_loss: 3.0146 - val_acc: 0.4003\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.41369\n",
            "Epoch 15/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0572 - acc: 0.4954 - val_loss: 2.9891 - val_acc: 0.4107\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.41369\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.\n",
            "Epoch 16/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0575 - acc: 0.4966 - val_loss: 2.9849 - val_acc: 0.4115\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.41369\n",
            "Epoch 17/20\n",
            "390/390 [==============================] - 483s 1s/step - loss: 2.0514 - acc: 0.4961 - val_loss: 2.9803 - val_acc: 0.4054\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.41369\n",
            "Epoch 18/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0535 - acc: 0.4946 - val_loss: 3.0343 - val_acc: 0.4053\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.41369\n",
            "Epoch 19/20\n",
            "390/390 [==============================] - 481s 1s/step - loss: 2.0454 - acc: 0.4965 - val_loss: 2.9948 - val_acc: 0.4076\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.41369\n",
            "Epoch 20/20\n",
            "390/390 [==============================] - 480s 1s/step - loss: 2.0525 - acc: 0.4958 - val_loss: 3.0210 - val_acc: 0.4031\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.41369\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.1622778678900043e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fe7f451d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXRy8rZkV4Um",
        "colab_type": "text"
      },
      "source": [
        "## **Switch to 64x64 images as now we have validation-accuracy of 41%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KHOSDrJSbi8",
        "colab_type": "code",
        "outputId": "12c3e726-4cc9-4152-df3d-e3b9fea4d2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "img_rows, img_cols = 64,64\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR, \n",
        "        target_size=(img_rows,img_cols),\n",
        "        color_mode = 'rgb',\n",
        "        batch_size=64,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed = 42)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "        val_data, \n",
        "        directory='./tiny-imagenet-200/val/images/', \n",
        "        x_col='File', y_col='Class', \n",
        "        target_size=(img_rows, img_cols),\n",
        "        color_mode='rgb', \n",
        "        class_mode='categorical',\n",
        "        batch_size=64, \n",
        "        shuffle=True, \n",
        "        seed=42)\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_TEST=validation_generator.n//validation_generator.batch_size\n",
        "\n",
        "model = load_model('/content/drive/My Drive/dataset/weights.epochs:009-val_acc:0.414.hdf5')\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_TEST,\n",
        "                    epochs=10,\n",
        "                    callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "1562/1562 [==============================] - 1908s 1s/step - loss: 0.1301 - acc: 0.9605 - val_loss: 3.9809 - val_acc: 0.4734\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.47336, saving model to /content/drive/My Drive/dataset/weights.epochs:001-val_acc:0.473.hdf5\n",
            "Epoch 2/10\n",
            "1562/1562 [==============================] - 1897s 1s/step - loss: 0.1035 - acc: 0.9674 - val_loss: 4.0993 - val_acc: 0.4729\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.47336\n",
            "Epoch 3/10\n",
            "1562/1562 [==============================] - 1893s 1s/step - loss: 0.0946 - acc: 0.9697 - val_loss: 4.2864 - val_acc: 0.4598\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.47336\n",
            "Epoch 4/10\n",
            "1562/1562 [==============================] - 1891s 1s/step - loss: 0.0898 - acc: 0.9711 - val_loss: 4.3292 - val_acc: 0.4738\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.47336 to 0.47383, saving model to /content/drive/My Drive/dataset/weights.epochs:004-val_acc:0.474.hdf5\n",
            "Epoch 5/10\n",
            "1562/1562 [==============================] - 1897s 1s/step - loss: 0.0802 - acc: 0.9741 - val_loss: 4.5228 - val_acc: 0.4605\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.47383\n",
            "Epoch 6/10\n",
            "1562/1562 [==============================] - 1895s 1s/step - loss: 0.0740 - acc: 0.9760 - val_loss: 4.3184 - val_acc: 0.4828\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.47383 to 0.48279, saving model to /content/drive/My Drive/dataset/weights.epochs:006-val_acc:0.483.hdf5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
            "Epoch 7/10\n",
            "1562/1562 [==============================] - 1894s 1s/step - loss: 0.0270 - acc: 0.9925 - val_loss: 4.2157 - val_acc: 0.5005\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.48279 to 0.50050, saving model to /content/drive/My Drive/dataset/weights.epochs:007-val_acc:0.501.hdf5\n",
            "Epoch 8/10\n",
            "1562/1562 [==============================] - 1892s 1s/step - loss: 0.0129 - acc: 0.9975 - val_loss: 4.3002 - val_acc: 0.5033\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.50050 to 0.50332, saving model to /content/drive/My Drive/dataset/weights.epochs:008-val_acc:0.503.hdf5\n",
            "Epoch 9/10\n",
            "1562/1562 [==============================] - 1894s 1s/step - loss: 0.0158 - acc: 0.9965 - val_loss: 4.4623 - val_acc: 0.4872\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.50332\n",
            "Epoch 10/10\n",
            "1562/1562 [==============================] - 1895s 1s/step - loss: 0.0151 - acc: 0.9967 - val_loss: 4.3661 - val_acc: 0.5012\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.50332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1f1bb51048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtUM1hPDU5hV",
        "colab_type": "text"
      },
      "source": [
        "## **Using Image-Augmentation  as val-acc on 64x64 is now around 50%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqSFhgsxr2wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using ImgAugmentation now as val-acc is around 50%\n",
        "\n",
        "def augmentors(batches, flip_value, crop_value, angle):\n",
        "  seq = iaa.Sequential([\n",
        "            iaa.Crop(percent=(0,crop_value)),\n",
        "            iaa.CoarseDropout(p=0.2, size_percent=0.05),\n",
        "            iaa.Affine(rotate=(-angle, angle)),\n",
        "            iaa.Multiply((0.4, 1.4)),\n",
        "            iaa.Sometimes(0.3, iaa.Fliplr(flip_value))]) # 50% flip / horizontal flip of only 30% of the images passed\n",
        "  while True:\n",
        "    batch_x, batch_y = next(batches)\n",
        "    batch_augmented = np.zeros((batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], img_channels)) \n",
        "    # NOTE: imgaug works on color images (3 channels). doesn't work on greyscale images with one channel\n",
        "    batch_augmented = seq.augment_images(batch_x) # calling ImgAug's augmentation on a batch of images\n",
        "    yield (batch_augmented, batch_y) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SqBSa2dE0qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CLR\n",
        "class CyclicLR(Callback):\n",
        "    \"source: https://github.com/bckenstler/CLR/blob/master/clr_callback.py\"\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHn-WMfzE7So",
        "colab_type": "code",
        "outputId": "686a09c2-67f8-4d31-dc13-e2ddf4daec94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Using ImgAugmentation now as val-acc is around 50%\n",
        "\n",
        "img_rows, img_cols = 64,64\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=[0.5, 1.30],\n",
        "                                   brightness_range=[0.5, 1.5],\n",
        "                                   rotation_range=35)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR, \n",
        "        target_size=(img_rows,img_cols),\n",
        "        color_mode = 'rgb',\n",
        "        batch_size=64,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed = 42)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "        val_data, \n",
        "        directory='./tiny-imagenet-200/val/images/', \n",
        "        x_col='File', y_col='Class', \n",
        "        target_size=(img_rows, img_cols),\n",
        "        color_mode='rgb', \n",
        "        class_mode='categorical',\n",
        "        batch_size=64, \n",
        "        shuffle=True, \n",
        "        seed=42)\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_TEST=validation_generator.n//validation_generator.batch_size\n",
        "print(\"STEP_SIZE_TRAIN\", STEP_SIZE_TRAIN)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "STEP_SIZE_TRAIN 1562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKcmdzMYDhGv",
        "colab_type": "text"
      },
      "source": [
        "### Using Cyclic Learning Rate from Keras (Policy: Triangular)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wTxW0xbFWIA",
        "colab_type": "code",
        "outputId": "8d2a5a42-0bcd-4fea-be10-5771bf19aef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1465
        }
      },
      "source": [
        "#Instantiate CLR (Policy = Triangular , Now with step size = 2*Iterations)\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.006,step_size=3162)\n",
        "callbacks_list = [checkpoint,clr]\n",
        "\n",
        "\n",
        "#Load back checkpointed model of 50% val accuracy\n",
        "model = load_model('/content/drive/My Drive/dataset/weights.epochs:008-val_acc:0.503.hdf5')\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_TEST,\n",
        "                    epochs=30,\n",
        "                    callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/30\n",
            "1562/1562 [==============================] - 1933s 1s/step - loss: 3.5663 - acc: 0.2305 - val_loss: 4.3746 - val_acc: 0.2202\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.22015, saving model to /content/drive/My Drive/dataset/weights.epochs:001-val_acc:0.220.hdf5\n",
            "Epoch 2/30\n",
            "1562/1562 [==============================] - 1924s 1s/step - loss: 3.3445 - acc: 0.2524 - val_loss: 4.2517 - val_acc: 0.1348\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.22015\n",
            "Epoch 3/30\n",
            "1562/1562 [==============================] - 1921s 1s/step - loss: 3.2406 - acc: 0.2726 - val_loss: 4.1710 - val_acc: 0.1829\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.22015\n",
            "Epoch 4/30\n",
            "1562/1562 [==============================] - 1921s 1s/step - loss: 2.8194 - acc: 0.3479 - val_loss: 2.9567 - val_acc: 0.3679\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.22015 to 0.36785, saving model to /content/drive/My Drive/dataset/weights.epochs:004-val_acc:0.368.hdf5\n",
            "Epoch 5/30\n",
            "1562/1562 [==============================] - 1922s 1s/step - loss: 2.6623 - acc: 0.3780 - val_loss: 4.0084 - val_acc: 0.2237\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.36785\n",
            "Epoch 6/30\n",
            "1562/1562 [==============================] - 1919s 1s/step - loss: 2.9602 - acc: 0.3210 - val_loss: 3.3640 - val_acc: 0.2738\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.36785\n",
            "Epoch 7/30\n",
            "1562/1562 [==============================] - 1913s 1s/step - loss: 2.9465 - acc: 0.3234 - val_loss: 3.0485 - val_acc: 0.3230\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.36785\n",
            "Epoch 8/30\n",
            "1562/1562 [==============================] - 1915s 1s/step - loss: 2.5961 - acc: 0.3915 - val_loss: 2.6541 - val_acc: 0.4163\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.36785 to 0.41626, saving model to /content/drive/My Drive/dataset/weights.epochs:008-val_acc:0.416.hdf5\n",
            "Epoch 9/30\n",
            "1562/1562 [==============================] - 1918s 1s/step - loss: 2.4236 - acc: 0.4246 - val_loss: 2.9729 - val_acc: 0.3637\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.41626\n",
            "Epoch 10/30\n",
            "1562/1562 [==============================] - 1912s 1s/step - loss: 2.6928 - acc: 0.3715 - val_loss: 3.1321 - val_acc: 0.3197\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.41626\n",
            "Epoch 11/30\n",
            "1562/1562 [==============================] - 1910s 1s/step - loss: 2.7471 - acc: 0.3611 - val_loss: 2.9117 - val_acc: 0.3764\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.41626\n",
            "Epoch 12/30\n",
            "1562/1562 [==============================] - 1911s 1s/step - loss: 2.4347 - acc: 0.4223 - val_loss: 2.6043 - val_acc: 0.4330\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.41626 to 0.43297, saving model to /content/drive/My Drive/dataset/weights.epochs:012-val_acc:0.433.hdf5\n",
            "Epoch 13/30\n",
            "1562/1562 [==============================] - 1909s 1s/step - loss: 2.2604 - acc: 0.4582 - val_loss: 3.0205 - val_acc: 0.3585\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.43297\n",
            "Epoch 14/30\n",
            "1562/1562 [==============================] - 1910s 1s/step - loss: 2.5062 - acc: 0.4073 - val_loss: 3.9734 - val_acc: 0.2482\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.43297\n",
            "Epoch 15/30\n",
            "1562/1562 [==============================] - 1911s 1s/step - loss: 2.5919 - acc: 0.3917 - val_loss: 3.4653 - val_acc: 0.3506\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.43297\n",
            "Epoch 16/30\n",
            "1562/1562 [==============================] - 1900s 1s/step - loss: 2.3027 - acc: 0.4496 - val_loss: 2.7526 - val_acc: 0.4208\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.43297\n",
            "Epoch 17/30\n",
            "1562/1562 [==============================] - 1905s 1s/step - loss: 2.1180 - acc: 0.4872 - val_loss: 2.7598 - val_acc: 0.4144\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.43297\n",
            "Epoch 18/30\n",
            "1562/1562 [==============================] - 1924s 1s/step - loss: 2.3630 - acc: 0.4364 - val_loss: 4.9300 - val_acc: 0.2395\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.43297\n",
            "Epoch 19/30\n",
            "1562/1562 [==============================] - 1924s 1s/step - loss: 2.4904 - acc: 0.4103 - val_loss: 3.4330 - val_acc: 0.3366\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.43297\n",
            "Epoch 20/30\n",
            "1562/1562 [==============================] - 1926s 1s/step - loss: 2.2116 - acc: 0.4674 - val_loss: 2.6337 - val_acc: 0.4447\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.43297 to 0.44475, saving model to /content/drive/My Drive/dataset/weights.epochs:020-val_acc:0.445.hdf5\n",
            "Epoch 21/30\n",
            " 965/1562 [=================>............] - ETA: 11:50 - loss: 1.9670 - acc: 0.5198"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSW9ZvN0Dxqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Instantiate CLR (Policy = Traingular)\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.006,step_size=1562)\n",
        "callbacks_list = [checkpoint,clr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls9BtIgXCJar",
        "colab_type": "text"
      },
      "source": [
        "### Colab disconnected resuming below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is_n4R1W_7JJ",
        "colab_type": "code",
        "outputId": "1f4c0a4f-1563-49cb-9bb8-112fce648f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "model = load_model('/content/drive/My Drive/dataset/weights.epochs:020-val_acc:0.445.hdf5')\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_TEST,\n",
        "                    epochs=20,\n",
        "                    callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "1562/1562 [==============================] - 1883s 1s/step - loss: 2.1890 - acc: 0.4720 - val_loss: 3.6805 - val_acc: 0.3230\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.32302, saving model to /content/drive/My Drive/dataset/weights.epochs:001-val_acc:0.323.hdf5\n",
            "Epoch 2/20\n",
            "1562/1562 [==============================] - 1870s 1s/step - loss: 2.2335 - acc: 0.4627 - val_loss: 2.4068 - val_acc: 0.4841\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.32302 to 0.48410, saving model to /content/drive/My Drive/dataset/weights.epochs:002-val_acc:0.484.hdf5\n",
            "Epoch 3/20\n",
            " 330/1562 [=====>........................] - ETA: 23:46 - loss: 1.9282 - acc: 0.5250"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6GBOQ5ChGT",
        "colab_type": "text"
      },
      "source": [
        "### Colab again disconnected resuming below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj0wDr2hDH1o",
        "colab_type": "code",
        "outputId": "b4a66544-79bc-4bf0-9557-1d83f00677b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1378
        }
      },
      "source": [
        "\n",
        "model = load_model('/content/drive/My Drive/dataset/weights.epochs:002-val_acc:0.484.hdf5')\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_TEST,\n",
        "                    epochs=20,\n",
        "                    callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1562/1562 [==============================] - 1909s 1s/step - loss: 1.9767 - acc: 0.5147 - val_loss: 4.1197 - val_acc: 0.3182\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.31821, saving model to /content/drive/My Drive/dataset/weights.epochs:001-val_acc:0.318.hdf5\n",
            "Epoch 2/20\n",
            "1562/1562 [==============================] - 1897s 1s/step - loss: 2.0574 - acc: 0.4974 - val_loss: 2.4356 - val_acc: 0.4837\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.31821 to 0.48370, saving model to /content/drive/My Drive/dataset/weights.epochs:002-val_acc:0.484.hdf5\n",
            "Epoch 3/20\n",
            "1562/1562 [==============================] - 1896s 1s/step - loss: 2.1185 - acc: 0.4859 - val_loss: 3.4495 - val_acc: 0.3316\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.48370\n",
            "Epoch 4/20\n",
            "1562/1562 [==============================] - 1896s 1s/step - loss: 2.1555 - acc: 0.4785 - val_loss: 2.7192 - val_acc: 0.4638\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.48370\n",
            "Epoch 5/20\n",
            "1562/1562 [==============================] - 1896s 1s/step - loss: 2.0703 - acc: 0.4967 - val_loss: 3.0045 - val_acc: 0.3821\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.48370\n",
            "Epoch 6/20\n",
            "1562/1562 [==============================] - 1898s 1s/step - loss: 2.1081 - acc: 0.4877 - val_loss: 2.6250 - val_acc: 0.4617\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.48370\n",
            "Epoch 7/20\n",
            "1562/1562 [==============================] - 1897s 1s/step - loss: 2.0225 - acc: 0.5058 - val_loss: 3.1896 - val_acc: 0.3306\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.48370\n",
            "Epoch 8/20\n",
            "1562/1562 [==============================] - 1897s 1s/step - loss: 2.0750 - acc: 0.4947 - val_loss: 2.7750 - val_acc: 0.4559\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.48370\n",
            "Epoch 9/20\n",
            "1562/1562 [==============================] - 1897s 1s/step - loss: 1.9836 - acc: 0.5134 - val_loss: 3.4536 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.48370\n",
            "Epoch 10/20\n",
            "1562/1562 [==============================] - 1898s 1s/step - loss: 2.0217 - acc: 0.5075 - val_loss: 2.7891 - val_acc: 0.4497\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.48370\n",
            "Epoch 11/20\n",
            "1562/1562 [==============================] - 1900s 1s/step - loss: 1.9326 - acc: 0.5266 - val_loss: 3.3267 - val_acc: 0.3812\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.48370\n",
            "Epoch 12/20\n",
            "1562/1562 [==============================] - 1902s 1s/step - loss: 1.9944 - acc: 0.5106 - val_loss: 2.6751 - val_acc: 0.4734\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.48370\n",
            "Epoch 13/20\n",
            "1562/1562 [==============================] - 1903s 1s/step - loss: 1.9111 - acc: 0.5302 - val_loss: 3.8192 - val_acc: 0.3308\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.48370\n",
            "Epoch 14/20\n",
            "1562/1562 [==============================] - 1900s 1s/step - loss: 1.9383 - acc: 0.5226 - val_loss: 2.6673 - val_acc: 0.4792\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.48370\n",
            "Epoch 15/20\n",
            "1562/1562 [==============================] - 1898s 1s/step - loss: 1.8502 - acc: 0.5422 - val_loss: 2.9985 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.48370\n",
            "Epoch 16/20\n",
            "1562/1562 [==============================] - 1901s 1s/step - loss: 1.9112 - acc: 0.5307 - val_loss: 2.6457 - val_acc: 0.4769\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.48370\n",
            "Epoch 17/20\n",
            "1562/1562 [==============================] - 1899s 1s/step - loss: 1.8335 - acc: 0.5449 - val_loss: 3.0129 - val_acc: 0.3868\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.48370\n",
            "Epoch 18/20\n",
            "1562/1562 [==============================] - 1902s 1s/step - loss: 1.8740 - acc: 0.5367 - val_loss: 2.4746 - val_acc: 0.5038\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.48370 to 0.50382, saving model to /content/drive/My Drive/dataset/weights.epochs:018-val_acc:0.504.hdf5\n",
            "Epoch 19/20\n",
            "1562/1562 [==============================] - 1900s 1s/step - loss: 1.7870 - acc: 0.5541 - val_loss: 3.6013 - val_acc: 0.3630\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.50382\n",
            "Epoch 20/20\n",
            "1562/1562 [==============================] - 1899s 1s/step - loss: 1.8560 - acc: 0.5402 - val_loss: 2.5318 - val_acc: 0.4978\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.50382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe11cc16da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L90zMKRbV0VL",
        "colab_type": "text"
      },
      "source": [
        "### Above cell-ouput shows max validation accuracy acheived so far is 50.382%"
      ]
    }
  ]
}